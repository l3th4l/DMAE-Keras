{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['http_proxy'] = 'http://proxy:80'\n",
    "os.environ['https_proxy'] = 'http://proxy:80'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-empirical-privacy in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (0.1.0)\n",
      "Requirement already satisfied: absl-py==1.*,>=1.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (1.4.0)\n",
      "Requirement already satisfied: immutabledict~=2.2 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (2.2.5)\n",
      "Requirement already satisfied: matplotlib~=3.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (3.9.2)\n",
      "Requirement already satisfied: numpy~=1.21 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (1.26.4)\n",
      "Requirement already satisfied: pandas~=1.4 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (1.5.1)\n",
      "Requirement already satisfied: scipy~=1.9 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (1.14.0)\n",
      "Requirement already satisfied: statsmodels==0.14.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (0.14.0)\n",
      "Requirement already satisfied: tensorflow~=2.4 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-privacy>=0.9.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (0.9.0)\n",
      "Requirement already satisfied: tf-models-official~=2.13 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-empirical-privacy) (2.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from scikit-learn==1.*,>=1.0->tensorflow-empirical-privacy) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from scikit-learn==1.*,>=1.0->tensorflow-empirical-privacy) (3.5.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from statsmodels==0.14.0->tensorflow-empirical-privacy) (0.5.6)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from statsmodels==0.14.0->tensorflow-empirical-privacy) (22.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from matplotlib~=3.3->tensorflow-empirical-privacy) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from pandas~=1.4->tensorflow-empirical-privacy) (2024.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (4.25.4)\n",
      "Requirement already satisfied: setuptools in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow~=2.4->tensorflow-empirical-privacy) (2.15.0)\n",
      "Requirement already satisfied: dm-tree==0.1.8 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (0.1.8)\n",
      "Requirement already satisfied: dp-accounting==0.4.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (0.4.3)\n",
      "Requirement already satisfied: tensorflow-probability~=0.22.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (0.22.1)\n",
      "Requirement already satisfied: attrs>=22 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from dp-accounting==0.4.3->tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (23.2.0)\n",
      "Requirement already satisfied: mpmath~=1.2 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from dp-accounting==0.4.3->tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (1.3.0)\n",
      "Requirement already satisfied: Cython in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (3.0.11)\n",
      "Requirement already satisfied: gin-config in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (2.142.0)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (1.6.17)\n",
      "Requirement already satisfied: oauth2client in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (4.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (4.10.0.84)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (9.0.0)\n",
      "Requirement already satisfied: pycocotools in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (2.0.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (6.0.2)\n",
      "Requirement already satisfied: sacrebleu in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (2.4.3)\n",
      "Requirement already satisfied: sentencepiece in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (0.2.0)\n",
      "Requirement already satisfied: seqeval in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (1.2.2)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (4.9.6)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (0.16.1)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (0.8.0)\n",
      "Requirement already satisfied: tensorflow-text~=2.15.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (2.15.0)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tf-models-official~=2.13->tensorflow-empirical-privacy) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow~=2.4->tensorflow-empirical-privacy) (0.42.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.34.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.19.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (4.1.1)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (2024.7.4)\n",
      "Requirement already satisfied: requests in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.2.1)\n",
      "Requirement already satisfied: bleach in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (6.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (3.0.3)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-hub>=0.6.0->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.15.1)\n",
      "Requirement already satisfied: decorator in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy>=0.9.0->tensorflow-empirical-privacy) (3.0.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from oauth2client->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.6.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from oauth2client->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.4.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from oauth2client->tf-models-official~=2.13->tensorflow-empirical-privacy) (4.9)\n",
      "Requirement already satisfied: portalocker in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.10.1)\n",
      "Requirement already satisfied: regex in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-empirical-privacy) (2024.7.24)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.9.0)\n",
      "Requirement already satisfied: colorama in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from sacrebleu->tf-models-official~=2.13->tensorflow-empirical-privacy) (5.3.0)\n",
      "Requirement already satisfied: click in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (8.1.7)\n",
      "Requirement already satisfied: promise in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (2.3)\n",
      "Requirement already satisfied: pyarrow in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.1.5)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (1.15.0)\n",
      "Requirement already satisfied: toml in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.10.2)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.5.1)\n",
      "Requirement already satisfied: etils>=1.9.1 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (1.9.2)\n",
      "Requirement already satisfied: fsspec in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (6.4.4)\n",
      "Requirement already satisfied: zipp in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from etils[enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (3.17.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (1.64.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (1.24.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official~=2.13->tensorflow-empirical-privacy) (5.5.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from requests->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (2.1.5)\n",
      "Requirement already satisfied: webencodings in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from bleach->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official~=2.13->tensorflow-empirical-privacy) (1.3)\n",
      "Requirement already satisfied: docstring-parser~=0.15 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (from simple-parsing->tensorflow-datasets->tf-models-official~=2.13->tensorflow-empirical-privacy) (0.16)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /apps/jupyterhub/jh3.1.1-py3.11/envs/tensorflow-2.15.0/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.4->tensorflow-empirical-privacy) (3.2.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow-estimator==2.15.0 in /home/hpc/rlvl/rlvl115h/.local/lib/python3.11/site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-empirical-privacy\n",
    "!pip install --upgrade tensorflow-estimator==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vJMiurZw83aO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 21:59:26.143067: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-07 21:59:26.143247: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-07 21:59:26.384633: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-07 21:59:26.872948: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-07 21:59:31.649918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "#from tensorflow.keras import ops\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelblocks.Augmentation import get_train_augmentation_model, get_test_augmentation_model, data_augmentation\n",
    "from modelblocks.Blocks import Patches, PatchEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3yzbxs3w9ne-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1024\n",
    "BATCH_SIZE = 256\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMsFYR2gFgXH"
   },
   "source": [
    "**Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5tL4yVT0-Gwr",
    "outputId": "64ea5e08-5273-4a16-d27d-f35ec4e68bc1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50000\n",
      "Testing samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 21:59:45.699643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10525 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n",
      "2024-09-07 21:59:45.700387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10525 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:83:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar100.load_data()\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Testing samples: {len(x_valid)}\")\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n",
    "\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices(x_valid)\n",
    "valid_ds = valid_ds.batch(BATCH_SIZE).prefetch(AUTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vIq7YzLQFZou",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The original DMAE Paper mentions that augmentation does not provide any significant performance improvement\n",
    "#But we'll still be adding it since that makes the model more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8bqwdDm_vaPM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 72\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "MASK_PROPORTION = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "J1OdtXZHu67i",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_train_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            L.Rescaling(1 / 255.0),\n",
    "            L.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "            L.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "            L.RandomFlip(\"horizontal\"),\n",
    "        ],\n",
    "        name=\"train_data_augmentation\",\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_test_augmentation_model():\n",
    "    model = keras.Sequential(\n",
    "        [L.Rescaling(1 / 255.0), L.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n",
    "        name=\"test_data_augmentation\",\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evtzPEfKwHcO"
   },
   "source": [
    "**Divide image into patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Cv4Yf2rwu8eF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Patches(L.Layer):\n",
    "    def __init__(self, patch_size=PATCH_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.resize = L.Reshape((-1, patch_size * patch_size * 3))\n",
    "\n",
    "    def call(self, images):\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patches = self.resize(patches)\n",
    "        return patches\n",
    "\n",
    "    def show_patched_image(self, images, patches):\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        print(f\"Index selected: {idx}.\")\n",
    "\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(keras.utils.array_to_img(images[idx]))\n",
    "        plt.suptitle('Original Image')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        n = int(np.sqrt(patches.shape[1]))\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.suptitle('Patches')\n",
    "        for i, patch in enumerate(patches[idx]):\n",
    "            ax = plt.subplot(n, n, i + 1)\n",
    "            patch_img = tf.reshape(patch, (self.patch_size, self.patch_size, 3))\n",
    "            plt.imshow(keras.utils.img_to_array(patch_img))\n",
    "            plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        return idx\n",
    "\n",
    "    def reconstruct_from_patch(self, patch):\n",
    "        num_patches = patch.shape[0]\n",
    "        n = int(np.sqrt(num_patches))\n",
    "        patch = tf.reshape(patch, (num_patches, self.patch_size, self.patch_size, 3))\n",
    "        rows = tf.split(patch, n, axis=0)\n",
    "        rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n",
    "        reconstructed = tf.concat(rows, axis=0)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "TOUr57j2wono",
    "outputId": "952a292a-ffeb-4262-d3d8-4d4b5a1e5d43",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index selected: 135.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 144 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_batch = next(iter(train_ds))\n",
    "\n",
    "augmentation_model = get_train_augmentation_model()\n",
    "augmented_images = augmentation_model(image_batch)\n",
    "\n",
    "patch_layer = Patches()\n",
    "patches = patch_layer(images=augmented_images)\n",
    "random_index = patch_layer.show_patched_image(images=augmented_images, patches=patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AT2d7TGww9Cq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LAYER_NORM_EPS = 1e-6\n",
    "ENC_PROJECTION_DIM = 64\n",
    "DEC_PROJECTION_DIM = 48\n",
    "ENC_NUM_HEADS = 4\n",
    "ENC_LAYERS = 6\n",
    "DEC_NUM_HEADS = 4\n",
    "DEC_LAYERS = (\n",
    "    2\n",
    ")\n",
    "ENC_TRANSFORMER_UNITS = [\n",
    "    ENC_PROJECTION_DIM * 2,\n",
    "    ENC_PROJECTION_DIM,\n",
    "]\n",
    "DEC_TRANSFORMER_UNITS = [\n",
    "    DEC_PROJECTION_DIM * 2,\n",
    "    DEC_PROJECTION_DIM,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5M7C7825xth1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "DOWNSTREAM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW6QldcZybUE"
   },
   "source": [
    "**Patch Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "U_YAtpYJxwbj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(L.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        projection_dim=ENC_PROJECTION_DIM,\n",
    "        mask_proportion=MASK_PROPORTION,\n",
    "        downstream=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_size = patch_size\n",
    "        self.projection_dim = projection_dim\n",
    "        self.mask_proportion = mask_proportion\n",
    "        self.downstream = downstream\n",
    "\n",
    "        self.mask_token = tf.Variable(\n",
    "            tf.random.normal([1, patch_size * patch_size * 3]), trainable=True\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        (_, self.num_patches, self.patch_area) = input_shape\n",
    "        self.projection = L.Dense(units=self.projection_dim)\n",
    "        self.position_embedding = L.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=self.projection_dim\n",
    "        )\n",
    "        self.num_mask = int(self.mask_proportion * self.num_patches)\n",
    "\n",
    "    def call(self, patches):\n",
    "        batch_size = tf.shape(patches)[0]\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n",
    "        pos_embeddings = tf.tile(\n",
    "            pos_embeddings, [batch_size, 1, 1]\n",
    "        )\n",
    "        patch_embeddings = (\n",
    "            self.projection(patches) + pos_embeddings\n",
    "        )\n",
    "        if self.downstream:\n",
    "            return patch_embeddings\n",
    "        else:\n",
    "            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n",
    "            unmasked_embeddings = tf.gather(\n",
    "                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )\n",
    "            unmasked_positions = tf.gather(\n",
    "                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n",
    "            )\n",
    "            masked_positions = tf.gather(\n",
    "                pos_embeddings, mask_indices, axis=1, batch_dims=1\n",
    "            )\n",
    "            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=0)\n",
    "            mask_tokens = tf.repeat(\n",
    "                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n",
    "            )\n",
    "\n",
    "            masked_embeddings = self.projection(mask_tokens) + masked_positions\n",
    "            return (\n",
    "                unmasked_embeddings,\n",
    "                masked_embeddings,\n",
    "                unmasked_positions,\n",
    "                mask_indices,\n",
    "                unmask_indices,\n",
    "            )\n",
    "\n",
    "    def get_random_indices(self, batch_size):\n",
    "        rand_indices = tf.argsort(\n",
    "            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n",
    "        )\n",
    "        mask_indices = rand_indices[:, : self.num_mask]\n",
    "        unmask_indices = rand_indices[:, self.num_mask :]\n",
    "        return mask_indices, unmask_indices\n",
    "\n",
    "    def generate_masked_image(self, patches, unmask_indices):\n",
    "        idx = np.random.choice(patches.shape[0])\n",
    "        patch = patches[idx]\n",
    "        unmask_index = unmask_indices[idx]\n",
    "        new_patch = np.zeros_like(patch)\n",
    "        count = 0\n",
    "        for i in range(unmask_index.shape[0]):\n",
    "            new_patch[unmask_index[i]] = patch[unmask_index[i]]\n",
    "        return new_patch, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9p8czLJcoLH"
   },
   "source": [
    "**Teacher Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "h_L6ebHfcp9D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder_ViT(L.Layer):\n",
    "    def __init__(self, num_patches, projection_dim, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = L.Dense(units=projection_dim)\n",
    "        self.position_embedding = L.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = ops.expand_dims(\n",
    "            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n",
    "        )\n",
    "        projected_patches = self.projection(patch)\n",
    "        encoded = projected_patches + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_patches\": self.num_patches})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yMlcj5tEgpDo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "teacher_config = {\n",
    "    'projection_dim': 128,\n",
    "    'num_heads' : 4,\n",
    "    'transformer_layers' : 8,\n",
    "    'mlp_head_units': [2048, 1024]\n",
    "}\n",
    "\n",
    "\n",
    "teacher_config['transformer_units'] = [\n",
    "    teacher_config['projection_dim'] * 2,\n",
    "    teacher_config['projection_dim']\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KSUu8gPab3t1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        L.Normalization(),\n",
    "        L.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        L.RandomFlip(\"horizontal\"),\n",
    "        L.RandomRotation(factor=0.02),\n",
    "        L.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "T23hnE4teygG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dense_projection(x, dropout_rate, hidden_units, name = None):\n",
    "    for units in hidden_units:\n",
    "        if name:\n",
    "            #print(name + '_' + str(units))\n",
    "            x = L.Dense(units, activation=tf.nn.gelu, name = name + '_' + str(units))(x)\n",
    "        else:\n",
    "            x = L.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = L.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "5VnU__GJgWD5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import name\n",
    "def create_vit_classifier():\n",
    "    inputs = keras.Input(shape=INPUT_SHAPE)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(PATCH_SIZE)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(NUM_PATCHES, teacher_config['projection_dim'], downstream=True)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for i in range(teacher_config['transformer_layers']):\n",
    "        # Layer normalization 1.\n",
    "        x1 = L.LayerNormalization(epsilon=1e-6, name = f\"vit_lnorm_{i}__1\")(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads=teacher_config['num_heads'], key_dim=teacher_config['projection_dim'], dropout=0.1,\n",
    "            name=f\"vit_attention_{i}\"\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = L.Add(name = f\"vit_add_{i}\")([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = L.LayerNormalization(epsilon=1e-6, name = f\"vit_lnorm_{i}__2\")(x2)\n",
    "        # MLP.\n",
    "        x3 = dense_projection(x3, hidden_units=teacher_config['transformer_units'], dropout_rate=0.1, name = f\"vit_dense_{i}\")\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = L.Add(name = f\"vit_add_{i}_2\")([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = L.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = L.Flatten()(representation)\n",
    "    representation = L.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = dense_projection(representation, hidden_units=teacher_config['mlp_head_units'], dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    num_classes = 100\n",
    "    logits = L.Dense(100)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    fe_model = keras.Model(inputs=inputs, outputs=model.get_layer(\"vit_dense_5_128\").output, name=\"vit_interim\")\n",
    "    return model, fe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxJIMwSkjO92",
    "outputId": "3396c1ab-5e94-4b25-ebb1-f823bd7c62fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_classifier, vit_interim = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_classifier.compile(\n",
    "    optimizer='adam',\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\n",
    "        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "        keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fec3a200cd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.load_weights('./weights_teacher/vit_128_v1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for layer_ in vit_classifier.layers:\n",
    "    layer_.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 13s 31ms/step - loss: 2.2016 - accuracy: 0.5206 - top-5-accuracy: 0.7848\n",
      "Test accuracy: 52.06%\n",
      "Test top 5 accuracy: 78.48%\n"
     ]
    }
   ],
   "source": [
    "_, accuracy, top_5_accuracy = vit_classifier.evaluate(x_valid, y_valid)\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "d5pFh8uGwvRO",
    "outputId": "226f21ee-fa72-4aa9-ca13-b25c97bb8f67",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=0.001, weight_decay=0.0001\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    #checkpoint_filepath = \"/chkpt/checkpoint.weights.h5\"\n",
    "    #checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_filepath,\n",
    "    #    monitor=\"val_accuracy\",\n",
    "    #    save_best_only=True,\n",
    "    #    save_weights_only=True,\n",
    "    #)\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=75,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[],\n",
    "    )\n",
    "\n",
    "    #model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_valid, y_valid)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "#vit_classifier = create_vit_classifier()\n",
    "#history = run_experiment(vit_classifier)\n",
    "\n",
    "\n",
    "def plot_history(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#plot_history(\"loss\")\n",
    "#plot_history(\"top-5-accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vit_classifier.save_weights('./weights_teacher/vit_128_v1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_ in vit_classifier.layers:\n",
    "    layer_.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorboard\n",
    "tensorboard.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXw-hlmhyiLj"
   },
   "source": [
    "**MAE Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "awqwXlzozlQr"
   },
   "outputs": [],
   "source": [
    "def create_encoder(num_heads=ENC_NUM_HEADS, num_layers=ENC_LAYERS, _teacher_config = teacher_config):\n",
    "    inputs = L.Input((None, ENC_PROJECTION_DIM))\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        x1 = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm1_{i}_enc\")(x)\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1,\n",
    "            name=f\"mae_attention_{i}_enc\"\n",
    "        )(x1, x1)\n",
    "        x2 = L.Add(name = f\"mae_add1_{i}_enc\")([attention_output, x])\n",
    "        x3 = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm2_{i}_enc\")(x2)\n",
    "        x3 = dense_projection(x3, hidden_units=ENC_TRANSFORMER_UNITS, dropout_rate=0.1, name = f\"mae_dense_{i}_enc\")\n",
    "        x = L.Add(name = f\"mae_add2_{i}_enc\")([x3, x2])\n",
    "\n",
    "    outputs = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm_out\")(x)\n",
    "    main_model = keras.Model(inputs, outputs, name=\"mae_encoder\")\n",
    "\n",
    "    #project intermediate output to teacher dimensions\n",
    "    attention_output = main_model.get_layer(\"mae_attention_4_enc\").output\n",
    "    out_project = dense_projection(outputs, hidden_units=teacher_config['transformer_units'], dropout_rate=0.01, name = f\"mae_dense_project\")\n",
    "\n",
    "    intermediate_layer_model = keras.Model(inputs=main_model.input, outputs=out_project, name=\"mae_encoder_interim\")\n",
    "    return main_model, intermediate_layer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "k1kt6-K3zobp"
   },
   "outputs": [],
   "source": [
    "def create_decoder(\n",
    "    num_layers=DEC_LAYERS, num_heads=DEC_NUM_HEADS, image_size=IMAGE_SIZE\n",
    "):\n",
    "    inputs = L.Input((NUM_PATCHES, ENC_PROJECTION_DIM))\n",
    "    x = L.Dense(DEC_PROJECTION_DIM)(inputs)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        x1 = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm1_{i}_dec\")(x)\n",
    "        attention_output = L.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=DEC_PROJECTION_DIM, dropout=0.1, name = f\"mae_attention_{i}_dec\"\n",
    "        )(x1, x1)\n",
    "        x2 = L.Add(name = f\"mae_add1_{i}_dec\")([attention_output, x])\n",
    "        x3 = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm2_{i}_dec\")(x2)\n",
    "        x3 = dense_projection(x3, hidden_units=DEC_TRANSFORMER_UNITS, dropout_rate=0.1, name = f\"mae_dense_{i}_dec\")\n",
    "        x = L.Add(name = f\"mae_add2_{i}_dec\")([x3, x2])\n",
    "\n",
    "    x = L.LayerNormalization(epsilon=LAYER_NORM_EPS, name = f\"mae_lnorm_out_dec\")(x)\n",
    "    x = L.Flatten(name = f\"mae_flatten_out_dec\")(x)\n",
    "    pre_final = L.Dense(units=image_size * image_size * 3, activation=\"sigmoid\")(x)\n",
    "    outputs = L.Reshape((image_size, image_size, 3))(pre_final)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"mae_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_PyoQRZS0Bda"
   },
   "outputs": [],
   "source": [
    "class MaskedAutoencoder(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_augmentation_model,\n",
    "        test_augmentation_model,\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        decoder,\n",
    "        encoder_interim,\n",
    "        teacher_model=vit_interim,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_augmentation_model = train_augmentation_model\n",
    "        self.test_augmentation_model = test_augmentation_model\n",
    "        self.patch_layer = patch_layer\n",
    "        self.patch_encoder = patch_encoder\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder_interim = encoder_interim\n",
    "        self.teacher_model = teacher_model\n",
    "\n",
    "    def calculate_loss(self, images, test=False):\n",
    "        if test:\n",
    "            augmented_images = self.test_augmentation_model(images)\n",
    "        else:\n",
    "            augmented_images = self.train_augmentation_model(images)\n",
    "\n",
    "        patches = self.patch_layer(augmented_images)\n",
    "        (\n",
    "            unmasked_embeddings,\n",
    "            masked_embeddings,\n",
    "            unmasked_positions,\n",
    "            mask_indices,\n",
    "            unmask_indices,\n",
    "        ) = self.patch_encoder(patches)\n",
    "\n",
    "        encoder_outputs = self.encoder(unmasked_embeddings)\n",
    "        encoder_outputs = encoder_outputs + unmasked_positions\n",
    "        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n",
    "        decoder_outputs = self.decoder(decoder_inputs)\n",
    "        decoder_patches = self.patch_layer(decoder_outputs)\n",
    "\n",
    "        loss_patch = tf.gather(patches, mask_indices, axis=1, batch_dims=1)\n",
    "        loss_output = tf.gather(decoder_patches, mask_indices, axis=1, batch_dims=1)\n",
    "\n",
    "        #intermediate output\n",
    "        #self.patch_layer.downstream = True\n",
    "        #fe_patches = self.patch_layer(augmented_images)\n",
    "        #fe_patch_embeddings = self.patch_encoder(fe_patches)\n",
    "\n",
    "        #print('patch', patches , fe_patches)\n",
    "        #print('patch_embeddings', fe_patch_embeddings)\n",
    "\n",
    "\n",
    "        #print(patch_layer.downstream)\n",
    "\n",
    "        self.patch_encoder.downstream = True\n",
    "        intermediate_embeddings = self.patch_encoder(patches)\n",
    "        intermediate_output = self.encoder_interim(intermediate_embeddings)\n",
    "\n",
    "        self.patch_encoder.downstream = False\n",
    "\n",
    "        #intermediate_output = tf.concat([intermediate_output, masked_embeddings], axis=1)\n",
    "\n",
    "        #intermediate teacher output\n",
    "        teacher_output = self.teacher_model(images)\n",
    "\n",
    "        #distillation loss\n",
    "        \n",
    "        loss_distill = self.compiled_loss(intermediate_output, teacher_output)\n",
    "\n",
    "        #self.patch_layer.downstream = False\n",
    "\n",
    "        total_loss = loss_distill #+ self.compiled_loss(loss_patch, loss_output) + loss_distill\n",
    "\n",
    "        return total_loss, loss_patch, loss_output, loss_distill\n",
    "\n",
    "    def train_step(self, images):\n",
    "        with tf.GradientTape() as tape:\n",
    "            total_loss, loss_patch, loss_output, loss_distill = self.calculate_loss(images)\n",
    "\n",
    "        train_vars = [\n",
    "            self.train_augmentation_model.trainable_variables,\n",
    "            self.patch_layer.trainable_variables,\n",
    "            self.patch_encoder.trainable_variables,\n",
    "            self.encoder.trainable_variables,\n",
    "            self.decoder.trainable_variables,\n",
    "        ]\n",
    "        grads = tape.gradient(total_loss, train_vars)\n",
    "        tv_list = []\n",
    "        for (grad, var) in zip(grads, train_vars):\n",
    "            for g, v in zip(grad, var):\n",
    "                tv_list.append((g, v))\n",
    "        self.optimizer.apply_gradients(tv_list)\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, images):\n",
    "        total_loss, loss_patch, loss_output, loss_distill = self.calculate_loss(images, test=True)\n",
    "        self.compiled_metrics.update_state(loss_patch, loss_output, loss_distill)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EXAB-TP4YZ9"
   },
   "source": [
    "**Monitoring Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "d43YX1x54b3T"
   },
   "outputs": [],
   "source": [
    "test_images = next(iter(valid_ds))\n",
    "\n",
    "\n",
    "class TrainMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, epoch_interval=None):\n",
    "        self.epoch_interval = epoch_interval\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None, base_path = './weights_distil'):\n",
    "        if self.epoch_interval and epoch % self.epoch_interval == 0:\n",
    "            test_augmented_images = self.model.test_augmentation_model(test_images)\n",
    "            test_patches = self.model.patch_layer(test_augmented_images)\n",
    "            (\n",
    "                test_unmasked_embeddings,\n",
    "                test_masked_embeddings,\n",
    "                test_unmasked_positions,\n",
    "                test_mask_indices,\n",
    "                test_unmask_indices,\n",
    "            ) = self.model.patch_encoder(test_patches)\n",
    "            test_encoder_outputs = self.model.encoder(test_unmasked_embeddings)\n",
    "            test_encoder_outputs = test_encoder_outputs + test_unmasked_positions\n",
    "            test_decoder_inputs = tf.concat(\n",
    "                [test_encoder_outputs, test_masked_embeddings], axis=1\n",
    "            )\n",
    "            test_decoder_outputs = self.model.decoder(test_decoder_inputs)\n",
    "            \n",
    "            #save model checkpoint\n",
    "            if not os.path.isdir(base_path + ('/_chk_a%i'%(epoch))):\n",
    "                os.mkdir(base_path + ('/_chk_a%i'%(epoch)))\n",
    "            self.model.save_weights(base_path + ('/_chk_a%i/model_checkpoint.ckpt'%(epoch)))\n",
    "            \n",
    "\n",
    "            # Intermediate layer output\n",
    "            #self.patch_encoder.downstream = True\n",
    "            #intermediate_layer = self.model.encoder_interim(test_unmasked_embeddings)\n",
    "            #intermediate_layer = intermediate_layer #+ test_unmasked_positions\n",
    "\n",
    "            # Show a maksed patch image.\n",
    "            test_masked_patch, idx = self.model.patch_encoder.generate_masked_image(\n",
    "                test_patches, test_unmask_indices\n",
    "            )\n",
    "            print(f\"\\nIdx chosen: {idx}\")\n",
    "            original_image = test_augmented_images[idx]\n",
    "            masked_image = self.model.patch_layer.reconstruct_from_patch(\n",
    "                test_masked_patch\n",
    "            )\n",
    "            reconstructed_image = test_decoder_outputs[idx]\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "            ax[0].imshow(original_image)\n",
    "            ax[0].set_title(f\"Original: {epoch:03d}\")\n",
    "\n",
    "            ax[1].imshow(masked_image)\n",
    "            ax[1].set_title(f\"Masked: {epoch:03d}\")\n",
    "\n",
    "            ax[2].imshow(reconstructed_image)\n",
    "            ax[2].set_title(f\"Resonstructed: {epoch:03d}\")\n",
    "\n",
    "\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORswAyjF43zX",
    "outputId": "fa7e754c-6721-4363-90a3-fa91286aa55b"
   },
   "outputs": [],
   "source": [
    "train_augmentation_model = get_train_augmentation_model()\n",
    "test_augmentation_model = get_test_augmentation_model()\n",
    "patch_layer = Patches()\n",
    "patch_encoder = PatchEncoder()\n",
    "encoder, enc_interim = create_encoder()\n",
    "decoder = create_decoder()\n",
    "\n",
    "mae_model = MaskedAutoencoder(\n",
    "    train_augmentation_model=train_augmentation_model,\n",
    "    test_augmentation_model=test_augmentation_model,\n",
    "    patch_layer=patch_layer,\n",
    "    patch_encoder=patch_encoder,\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    encoder_interim=enc_interim\n",
    ")\n",
    "\n",
    "\n",
    "mae_model.compile(\n",
    "    optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mae_model.load_weights('./weights_notrain/_chk_70/model_checkpoint.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "n2gJVZ3AkduZ"
   },
   "outputs": [],
   "source": [
    "#vit_interim.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "VlLZyVqYj9yl"
   },
   "outputs": [],
   "source": [
    "#mae_model.encoder_interim.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "YNbitNOz529f",
    "outputId": "f641a248-e625-455f-f3ca-e0fce58c2913"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nkeras.utils.plot_model(encoder, show_shapes=True, to_file=\"encoder.png\")\\nkeras.utils.plot_model(decoder, show_shapes=True, to_file=\"decoder.png\")\\nfig, ax = plt.subplots(1, 2, figsize=(20, 60))\\nax[0].imshow(plt.imread(\\'encoder.png\\'))\\nax[0].set_title(\\'Encoder\\', fontsize=12)\\nax[0].axis(\"off\")\\nax[1].imshow(plt.imread(\\'decoder.png\\'))\\nax[1].set_title(\\'Decoder\\', fontsize=12)\\nax[1].axis(\"off\");\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "keras.utils.plot_model(encoder, show_shapes=True, to_file=\"encoder.png\")\n",
    "keras.utils.plot_model(decoder, show_shapes=True, to_file=\"decoder.png\")\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 60))\n",
    "ax[0].imshow(plt.imread('encoder.png'))\n",
    "ax[0].set_title('Encoder', fontsize=12)\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].imshow(plt.imread('decoder.png'))\n",
    "ax[1].set_title('Decoder', fontsize=12)\n",
    "ax[1].axis(\"off\");\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack_distil1.csv\t   dstr_dmae_l\n",
      "attack_distil2.csv\t   dstr_dmae_l1\n",
      "attack_distil3.csv\t   dstr_dmae_l2\n",
      "attack_distil4.csv\t   dstr_dmae_l3\n",
      "attack_distil.csv\t   dstr_dmae_l4\n",
      "attack_distil_dmae3.csv    dstr_dmae_l5\n",
      "attack_distil_dmae4.csv    dstr_dp\n",
      "attack_distil_dmae5.csv    dstr.index\n",
      "attack_distil_dmae.csv\t   dstr_notrain2\n",
      "attack_distil_dmae_l1.csv  dstr_notrain3\n",
      "attack_distil_dmae_l2.csv  dstr_notrain4\n",
      "attack_distil_dmae_l4.csv  dstr_notrain_l2\n",
      "attack_distil_dmae_l5.csv  dstr_notrain_l3\n",
      "attack_distil_dmae_l.csv   dstr_notrain_l4\n",
      "attack_distil_dp.csv\t   MAE_Keras_2_1.ipynb\n",
      "attack_notrain1.csv\t   README.md\n",
      "attack_notrain2.csv\t   tblogs\n",
      "attack_notrain3.csv\t   tboard_logs.zip\n",
      "attack_notrain4.csv\t   test\n",
      "attack_notrain.txt\t   Untitled.ipynb\n",
      "checkpoint\t\t   utils\n",
      "DMAE_Keras.ipynb\t   vit_128_v1.ckpt.data-00000-of-00001\n",
      "dstr\t\t\t   vit_128_v1.ckpt.index\n",
      "dstr2\t\t\t   vit_128_v1.weights.h5\n",
      "dstr3\t\t\t   ViT_Keras.ipynb\n",
      "dstr4\t\t\t   weights_distil\n",
      "dstr.data-00000-of-00001   weights_mae\n",
      "dstr_dmae\t\t   weights_mae_nodistil\n",
      "dstr_dmae1\t\t   weights_notrain\n",
      "dstr_dmae2\t\t   weights_teacher\n",
      "dstr_dmae3\t\t   weights_teacher.zip\n",
      "dstr_dmae4\t\t   xyz.py\n",
      "dstr_dmae5\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logdir=\"./tblogs/mae/_distil_dmae\"\n",
    "#if not os.path.isdir(logdir):\n",
    "#                os.mkdir(logdir)\n",
    "#tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "_tSJRO1z2Ls4",
    "outputId": "016e36a9-8e31-4904-a8e5-a85cc8f4d8cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 14s 205ms/step - loss: 372.7503 - mae: 0.2601\n",
      "Loss: 372.75\n",
      "MAE: 0.26\n"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=25, verbose=1, restore_best_weights=True)\n",
    "rlp = keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "\n",
    "#history = mae_model.fit(\n",
    "#    train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=[TrainMonitor(epoch_interval=5), es, rlp, tensorboard_callback],\n",
    "#)\n",
    "\n",
    "loss, mae = mae_model.evaluate(valid_ds)\n",
    "print(f\"Loss: {loss:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for layer_ in mae_model.layers:\n",
    "#    layer_.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nattn_only = True\\nfound_attn = False\\nfor layer_ in mae_model.encoder.layers:\\n    if layer_.name == 'mae_attention_2_enc':\\n        found_attn = True\\n    if attn_only:\\n        if not found_attn:\\n            layer_.trainable = False\\n        else:\\n            layer_.trainable = True\\n    else:\\n        layer_.trainable = False\\n\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "attn_only = True\n",
    "found_attn = False\n",
    "for layer_ in mae_model.encoder.layers:\n",
    "    if layer_.name == 'mae_attention_2_enc':\n",
    "        found_attn = True\n",
    "    if attn_only:\n",
    "        if not found_attn:\n",
    "            layer_.trainable = False\n",
    "        else:\n",
    "            layer_.trainable = True\n",
    "    else:\n",
    "        layer_.trainable = False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mae_encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, None, 64)]           0         []                            \n",
      "                                                                                                  \n",
      " mae_lnorm1_0_enc (LayerNor  (None, None, 64)             128       ['input_2[0][0]']             \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_0_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_0_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_0_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_0_enc (Add)        (None, None, 64)             0         ['mae_attention_0_enc[0][0]', \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " mae_lnorm2_0_enc (LayerNor  (None, None, 64)             128       ['mae_add1_0_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_0_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_0_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)        (None, None, 128)            0         ['mae_dense_0_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_0_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_19[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)        (None, None, 64)             0         ['mae_dense_0_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_0_enc (Add)        (None, None, 64)             0         ['dropout_20[0][0]',          \n",
      "                                                                     'mae_add1_0_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm1_1_enc (LayerNor  (None, None, 64)             128       ['mae_add2_0_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_1_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_1_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_1_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_1_enc (Add)        (None, None, 64)             0         ['mae_attention_1_enc[0][0]', \n",
      "                                                                     'mae_add2_0_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm2_1_enc (LayerNor  (None, None, 64)             128       ['mae_add1_1_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_1_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_1_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)        (None, None, 128)            0         ['mae_dense_1_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_1_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_21[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_22 (Dropout)        (None, None, 64)             0         ['mae_dense_1_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_1_enc (Add)        (None, None, 64)             0         ['dropout_22[0][0]',          \n",
      "                                                                     'mae_add1_1_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm1_2_enc (LayerNor  (None, None, 64)             128       ['mae_add2_1_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_2_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_2_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_2_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_2_enc (Add)        (None, None, 64)             0         ['mae_attention_2_enc[0][0]', \n",
      "                                                                     'mae_add2_1_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm2_2_enc (LayerNor  (None, None, 64)             128       ['mae_add1_2_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_2_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_2_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_23 (Dropout)        (None, None, 128)            0         ['mae_dense_2_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_2_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_23[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_24 (Dropout)        (None, None, 64)             0         ['mae_dense_2_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_2_enc (Add)        (None, None, 64)             0         ['dropout_24[0][0]',          \n",
      "                                                                     'mae_add1_2_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm1_3_enc (LayerNor  (None, None, 64)             128       ['mae_add2_2_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_3_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_3_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_3_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_3_enc (Add)        (None, None, 64)             0         ['mae_attention_3_enc[0][0]', \n",
      "                                                                     'mae_add2_2_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm2_3_enc (LayerNor  (None, None, 64)             128       ['mae_add1_3_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_3_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_3_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_25 (Dropout)        (None, None, 128)            0         ['mae_dense_3_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_3_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_25[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_26 (Dropout)        (None, None, 64)             0         ['mae_dense_3_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_3_enc (Add)        (None, None, 64)             0         ['dropout_26[0][0]',          \n",
      "                                                                     'mae_add1_3_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm1_4_enc (LayerNor  (None, None, 64)             128       ['mae_add2_3_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_4_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_4_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_4_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_4_enc (Add)        (None, None, 64)             0         ['mae_attention_4_enc[0][0]', \n",
      "                                                                     'mae_add2_3_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm2_4_enc (LayerNor  (None, None, 64)             128       ['mae_add1_4_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_4_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_4_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_27 (Dropout)        (None, None, 128)            0         ['mae_dense_4_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_4_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_27[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_28 (Dropout)        (None, None, 64)             0         ['mae_dense_4_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_4_enc (Add)        (None, None, 64)             0         ['dropout_28[0][0]',          \n",
      "                                                                     'mae_add1_4_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm1_5_enc (LayerNor  (None, None, 64)             128       ['mae_add2_4_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_attention_5_enc (Multi  (None, None, 64)             66368     ['mae_lnorm1_5_enc[0][0]',    \n",
      " HeadAttention)                                                      'mae_lnorm1_5_enc[0][0]']    \n",
      "                                                                                                  \n",
      " mae_add1_5_enc (Add)        (None, None, 64)             0         ['mae_attention_5_enc[0][0]', \n",
      "                                                                     'mae_add2_4_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm2_5_enc (LayerNor  (None, None, 64)             128       ['mae_add1_5_enc[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " mae_dense_5_enc_128 (Dense  (None, None, 128)            8320      ['mae_lnorm2_5_enc[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_29 (Dropout)        (None, None, 128)            0         ['mae_dense_5_enc_128[0][0]'] \n",
      "                                                                                                  \n",
      " mae_dense_5_enc_64 (Dense)  (None, None, 64)             8256      ['dropout_29[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_30 (Dropout)        (None, None, 64)             0         ['mae_dense_5_enc_64[0][0]']  \n",
      "                                                                                                  \n",
      " mae_add2_5_enc (Add)        (None, None, 64)             0         ['dropout_30[0][0]',          \n",
      "                                                                     'mae_add1_5_enc[0][0]']      \n",
      "                                                                                                  \n",
      " mae_lnorm_out (LayerNormal  (None, None, 64)             128       ['mae_add2_5_enc[0][0]']      \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 499328 (1.90 MB)\n",
      "Trainable params: 499328 (1.90 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mae_model.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#mae_model.save_weights('./weights_mae/mae_70.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downstream Classification Tasks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Ae9RHB8e3zsv"
   },
   "outputs": [],
   "source": [
    "train_augmentation_model = mae_model.train_augmentation_model\n",
    "test_augmentation_model = mae_model.test_augmentation_model\n",
    "\n",
    "patch_layer = mae_model.patch_layer\n",
    "patch_encoder = mae_model.patch_encoder\n",
    "patch_encoder.downstream = True\n",
    "\n",
    "encoder = mae_model.encoder\n",
    "dropout_rate= 0.5\n",
    "downstream_model = keras.Sequential(\n",
    "    [\n",
    "        L.Input((IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "        patch_layer,\n",
    "        patch_encoder,\n",
    "        encoder,\n",
    "        L.BatchNormalization(),\n",
    "        L.GlobalAveragePooling1D(),\n",
    "        L.Dense(2048, activation=keras.activations.gelu),\n",
    "        L.Dropout(dropout_rate),\n",
    "        L.Dense(1024, activation=keras.activations.gelu),\n",
    "        L.Dropout(dropout_rate),\n",
    "        L.Dense(10),\n",
    "    ],\n",
    "    name=\"downstream_model1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(images, labels, is_train=True):\n",
    "    if is_train:\n",
    "        augmentation_model = train_augmentation_model\n",
    "    else:\n",
    "        augmentation_model = test_augmentation_model\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE).map(\n",
    "        lambda x, y: (augmentation_model(x), y), num_parallel_calls=AUTO\n",
    "    )\n",
    "    return dataset.prefetch(AUTO)\n",
    "\n",
    "(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar10.load_data()\n",
    "train_ds = prepare_data(x_train, y_train)\n",
    "valid_ds = prepare_data(x_valid, y_valid, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logdir_dstr=\"./tblogs/dstr/_notrain5\"\n",
    "if not os.path.isdir(logdir_dstr):\n",
    "                os.mkdir(logdir_dstr)\n",
    "tensorboard_callback_dstr = keras.callbacks.TensorBoard(log_dir=logdir_dstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dstr=\"./dstr_notrain5\"\n",
    "flname = '/mdl.ckpt'\n",
    "if not os.path.isdir(ckpt_dstr):\n",
    "                os.mkdir(ckpt_dstr)\n",
    "        \n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_dstr+flname,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dstr_l=\"./dstr_notrain_l5\"\n",
    "flname = '/mdl.ckpt'\n",
    "if not os.path.isdir(ckpt_dstr_l):\n",
    "                os.mkdir(ckpt_dstr_l)\n",
    "        \n",
    "model_checkpoint_callback1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=ckpt_dstr_l+flname,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=0.0001\n",
    "#weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_privacy import DPKerasAdamOptimizer as DPAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 0.6\n",
    "num_microbatches = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=learning_rate, amsgrad=True#, weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "#DP Adam \n",
    "\n",
    "#optimizer = DPAdam(\n",
    "#    l2_norm_clip=l2_norm_clip, \n",
    "#    noise_multiplier=noise_multiplier,\n",
    "#    num_microbatches=num_microbatches, \n",
    "#    learning_rate=learning_rate, \n",
    "#    amsgrad=True\n",
    "#)\n",
    "\n",
    "downstream_model.compile(\n",
    "    optimizer=optimizer, loss=loss, metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['Variable:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-07 22:00:54.054373: I external/local_xla/xla/service/service.cc:168] XLA service 0x7feb59d70b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-07 22:00:54.054434: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-09-07 22:00:54.054447: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1\n",
      "2024-09-07 22:00:54.099583: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-07 22:00:54.180118: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1725739254.309018 4034538 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 71s 242ms/step - loss: 1.9068 - accuracy: 0.2790 - val_loss: 1.7106 - val_accuracy: 0.3548\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 46s 237ms/step - loss: 1.6294 - accuracy: 0.3872 - val_loss: 1.6546 - val_accuracy: 0.3946\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 47s 239ms/step - loss: 1.4983 - accuracy: 0.4436 - val_loss: 1.5053 - val_accuracy: 0.4644\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 48s 243ms/step - loss: 1.4007 - accuracy: 0.4847 - val_loss: 1.4001 - val_accuracy: 0.4914\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 46s 236ms/step - loss: 1.3233 - accuracy: 0.5165 - val_loss: 1.4393 - val_accuracy: 0.4915\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 1.2634 - accuracy: 0.5408 - val_loss: 1.3564 - val_accuracy: 0.5135\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 1.2162 - accuracy: 0.5607 - val_loss: 1.3105 - val_accuracy: 0.5392\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 1.1760 - accuracy: 0.5725 - val_loss: 1.5868 - val_accuracy: 0.4904\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 48s 243ms/step - loss: 1.1392 - accuracy: 0.5871 - val_loss: 1.2336 - val_accuracy: 0.5603\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 47s 242ms/step - loss: 1.1087 - accuracy: 0.5992 - val_loss: 1.1678 - val_accuracy: 0.5898\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 1.0776 - accuracy: 0.6126 - val_loss: 1.1914 - val_accuracy: 0.5886\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 45s 232ms/step - loss: 1.0563 - accuracy: 0.6194 - val_loss: 1.2317 - val_accuracy: 0.5741\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 47s 237ms/step - loss: 1.0360 - accuracy: 0.6266 - val_loss: 1.1814 - val_accuracy: 0.5958\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 47s 239ms/step - loss: 1.0188 - accuracy: 0.6352 - val_loss: 1.1620 - val_accuracy: 0.5949\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.9967 - accuracy: 0.6422 - val_loss: 1.1502 - val_accuracy: 0.6101\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 46s 235ms/step - loss: 0.9822 - accuracy: 0.6475 - val_loss: 1.1411 - val_accuracy: 0.6127\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 45s 231ms/step - loss: 0.9642 - accuracy: 0.6547 - val_loss: 1.0263 - val_accuracy: 0.6456\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.9513 - accuracy: 0.6608 - val_loss: 1.0603 - val_accuracy: 0.6408\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.9374 - accuracy: 0.6645 - val_loss: 1.0624 - val_accuracy: 0.6357\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.9251 - accuracy: 0.6701 - val_loss: 1.0353 - val_accuracy: 0.6485\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 45s 227ms/step - loss: 0.9091 - accuracy: 0.6742 - val_loss: 1.0270 - val_accuracy: 0.6558\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 43s 222ms/step - loss: 0.9008 - accuracy: 0.6797 - val_loss: 1.0666 - val_accuracy: 0.6399\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 46s 234ms/step - loss: 0.8850 - accuracy: 0.6844 - val_loss: 0.9768 - val_accuracy: 0.6559\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 43s 218ms/step - loss: 0.8726 - accuracy: 0.6897 - val_loss: 0.9978 - val_accuracy: 0.6534\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 45s 228ms/step - loss: 0.8652 - accuracy: 0.6944 - val_loss: 1.0239 - val_accuracy: 0.6586\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.8572 - accuracy: 0.6954 - val_loss: 0.9984 - val_accuracy: 0.6652\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 45s 230ms/step - loss: 0.8440 - accuracy: 0.6981 - val_loss: 0.9793 - val_accuracy: 0.6693\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.8352 - accuracy: 0.7020 - val_loss: 1.0312 - val_accuracy: 0.6552\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.8282 - accuracy: 0.7060 - val_loss: 0.9895 - val_accuracy: 0.6705\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.8177 - accuracy: 0.7100 - val_loss: 0.9976 - val_accuracy: 0.6691\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 46s 235ms/step - loss: 0.8120 - accuracy: 0.7118 - val_loss: 0.9640 - val_accuracy: 0.6798\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.8032 - accuracy: 0.7152 - val_loss: 0.9585 - val_accuracy: 0.6825\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.7934 - accuracy: 0.7196 - val_loss: 0.9482 - val_accuracy: 0.6844\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 46s 234ms/step - loss: 0.7853 - accuracy: 0.7190 - val_loss: 0.9280 - val_accuracy: 0.6891\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 43s 221ms/step - loss: 0.7819 - accuracy: 0.7203 - val_loss: 0.9618 - val_accuracy: 0.6802\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 46s 233ms/step - loss: 0.7682 - accuracy: 0.7281 - val_loss: 0.9239 - val_accuracy: 0.6950\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.7650 - accuracy: 0.7293 - val_loss: 0.9358 - val_accuracy: 0.6931\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 46s 236ms/step - loss: 0.7582 - accuracy: 0.7306 - val_loss: 0.9170 - val_accuracy: 0.6990\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.7475 - accuracy: 0.7340 - val_loss: 0.9630 - val_accuracy: 0.6863\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.7405 - accuracy: 0.7367 - val_loss: 0.9331 - val_accuracy: 0.6944\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.7386 - accuracy: 0.7377 - val_loss: 0.9487 - val_accuracy: 0.6905\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.7330 - accuracy: 0.7412 - val_loss: 0.9481 - val_accuracy: 0.6955\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.7226 - accuracy: 0.7440 - val_loss: 0.9755 - val_accuracy: 0.6813\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.7172 - accuracy: 0.7440 - val_loss: 0.9329 - val_accuracy: 0.6965\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.7171 - accuracy: 0.7451 - val_loss: 0.9483 - val_accuracy: 0.6937\n",
      "Epoch 46/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.7056 - accuracy: 0.7478 - val_loss: 0.9737 - val_accuracy: 0.6884\n",
      "Epoch 47/150\n",
      "196/196 [==============================] - 43s 221ms/step - loss: 0.7002 - accuracy: 0.7518 - val_loss: 0.9596 - val_accuracy: 0.6896\n",
      "Epoch 48/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6959 - accuracy: 0.7537 - val_loss: 0.9615 - val_accuracy: 0.6960\n",
      "Epoch 49/150\n",
      "196/196 [==============================] - 46s 236ms/step - loss: 0.6871 - accuracy: 0.7559 - val_loss: 0.9003 - val_accuracy: 0.7072\n",
      "Epoch 50/150\n",
      "196/196 [==============================] - 43s 221ms/step - loss: 0.6838 - accuracy: 0.7569 - val_loss: 0.9533 - val_accuracy: 0.6973\n",
      "Epoch 51/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.6793 - accuracy: 0.7580 - val_loss: 0.9328 - val_accuracy: 0.7020\n",
      "Epoch 52/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.6724 - accuracy: 0.7588 - val_loss: 0.9496 - val_accuracy: 0.7008\n",
      "Epoch 53/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6642 - accuracy: 0.7654 - val_loss: 0.9864 - val_accuracy: 0.6965\n",
      "Epoch 54/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.6645 - accuracy: 0.7625 - val_loss: 0.9412 - val_accuracy: 0.6957\n",
      "Epoch 55/150\n",
      "196/196 [==============================] - 43s 222ms/step - loss: 0.6565 - accuracy: 0.7670 - val_loss: 0.9379 - val_accuracy: 0.7045\n",
      "Epoch 56/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.6487 - accuracy: 0.7689 - val_loss: 0.9463 - val_accuracy: 0.7065\n",
      "Epoch 57/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6422 - accuracy: 0.7712 - val_loss: 1.0759 - val_accuracy: 0.6807\n",
      "Epoch 58/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.6444 - accuracy: 0.7709 - val_loss: 0.9807 - val_accuracy: 0.6954\n",
      "Epoch 59/150\n",
      "196/196 [==============================] - 45s 229ms/step - loss: 0.6377 - accuracy: 0.7733 - val_loss: 0.9009 - val_accuracy: 0.7166\n",
      "Epoch 60/150\n",
      "196/196 [==============================] - 44s 222ms/step - loss: 0.6340 - accuracy: 0.7733 - val_loss: 0.9592 - val_accuracy: 0.6977\n",
      "Epoch 61/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6284 - accuracy: 0.7761 - val_loss: 0.9562 - val_accuracy: 0.7018\n",
      "Epoch 62/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6240 - accuracy: 0.7786 - val_loss: 0.9538 - val_accuracy: 0.7025\n",
      "Epoch 63/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6130 - accuracy: 0.7824 - val_loss: 0.9910 - val_accuracy: 0.6964\n",
      "Epoch 64/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6172 - accuracy: 0.7796 - val_loss: 0.9889 - val_accuracy: 0.6976\n",
      "Epoch 65/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.6065 - accuracy: 0.7827 - val_loss: 0.9475 - val_accuracy: 0.7048\n",
      "Epoch 66/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.6083 - accuracy: 0.7811 - val_loss: 0.9860 - val_accuracy: 0.6945\n",
      "Epoch 67/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.5989 - accuracy: 0.7872 - val_loss: 0.9419 - val_accuracy: 0.7107\n",
      "Epoch 68/150\n",
      "196/196 [==============================] - 43s 217ms/step - loss: 0.5935 - accuracy: 0.7891 - val_loss: 0.9949 - val_accuracy: 0.6997\n",
      "Epoch 69/150\n",
      "196/196 [==============================] - 43s 220ms/step - loss: 0.5896 - accuracy: 0.7887 - val_loss: 0.9416 - val_accuracy: 0.7116\n",
      "Epoch 70/150\n",
      "196/196 [==============================] - 42s 216ms/step - loss: 0.5835 - accuracy: 0.7918 - val_loss: 1.0050 - val_accuracy: 0.6996\n",
      "Epoch 71/150\n",
      "196/196 [==============================] - 45s 227ms/step - loss: 0.5862 - accuracy: 0.7897 - val_loss: 0.9056 - val_accuracy: 0.7181\n",
      "Epoch 72/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.5745 - accuracy: 0.7944 - val_loss: 0.9587 - val_accuracy: 0.7127\n",
      "Epoch 73/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.5711 - accuracy: 0.7952 - val_loss: 0.9511 - val_accuracy: 0.7102\n",
      "Epoch 74/150\n",
      "196/196 [==============================] - 44s 224ms/step - loss: 0.5695 - accuracy: 0.7970 - val_loss: 0.9728 - val_accuracy: 0.7088\n",
      "Epoch 75/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.5648 - accuracy: 0.7969 - val_loss: 1.0330 - val_accuracy: 0.6976\n",
      "Epoch 76/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.5618 - accuracy: 0.8007 - val_loss: 0.9833 - val_accuracy: 0.7022\n",
      "Epoch 77/150\n",
      "196/196 [==============================] - 44s 223ms/step - loss: 0.5577 - accuracy: 0.8017 - val_loss: 0.9699 - val_accuracy: 0.7135\n",
      "Epoch 78/150\n",
      " 93/196 [=============>................] - ETA: 21s - loss: 0.5511 - accuracy: 0.8013"
     ]
    }
   ],
   "source": [
    "es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=25, verbose=1, restore_best_weights=True)\n",
    "rlp = keras.callbacks.ReduceLROnPlateau(patience=4, verbose=1)\n",
    "\n",
    "downstream_model.fit(train_ds, validation_data=valid_ds, epochs=150, callbacks=[tensorboard_callback_dstr, model_checkpoint_callback, model_checkpoint_callback1])\n",
    "\n",
    "loss, accuracy = downstream_model.evaluate(valid_ds)\n",
    "accuracy = round(accuracy * 100, 2)\n",
    "print(f\"Accuracy on the test set: {accuracy}%.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downstream_model.load_weights(\"./dstr_notrain5/mdl.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, accuracy = downstream_model.evaluate(valid_ds)\n",
    "accuracy = round(accuracy * 100, 2)\n",
    "print(f\"Accuracy on the test set: {accuracy}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Membership Inference Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug = test_augmentation_model(x_train)\n",
    "x_valid_aug = test_augmentation_model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predict on train...')\n",
    "logits_train = downstream_model.predict(x_train_aug)\n",
    "print('Predict on test...')\n",
    "logits_test = downstream_model.predict(x_valid_aug)\n",
    "\n",
    "print('Apply softmax to get probabilities from logits...')\n",
    "prob_train = tf.nn.softmax(logits_train, axis=-1)\n",
    "prob_test = tf.nn.softmax(logits_test)\n",
    "\n",
    "print('Compute losses...')\n",
    "cce = tf.keras.backend.categorical_crossentropy\n",
    "constant = tf.keras.backend.constant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train_onehot = keras.utils.to_categorical(y_train, num_classes = 10).astype(int)\n",
    "y_test_onehot = keras.utils.to_categorical(y_valid, num_classes = 10).astype(int)\n",
    "\n",
    "loss_train = cce(constant(y_train_onehot), constant(prob_train), from_logits=False).numpy()\n",
    "loss_test = cce(constant(y_test_onehot), constant(prob_test), from_logits=False).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train.flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "attack_input = AttackInputData(\n",
    "  logits_train = logits_train,\n",
    "  logits_test = logits_test,\n",
    "  loss_train = loss_train,\n",
    "  loss_test = loss_test,\n",
    "  labels_train = y_train.flatten().astype(int),\n",
    "  labels_test = y_valid.flatten().astype(int)\n",
    ")\n",
    "\n",
    "slicing_spec = SlicingSpec(\n",
    "    entire_dataset = True,\n",
    "    by_class = True,\n",
    "    by_percentiles = False,\n",
    "by_classification_correctness = True)\n",
    "\n",
    "attack_types = [\n",
    "    AttackType.THRESHOLD_ATTACK,\n",
    "    AttackType.MULTI_LAYERED_PERCEPTRON \n",
    "] \n",
    "\n",
    "attacks_result = mia.run_attacks(attack_input=attack_input,\n",
    "                                 slicing_spec=slicing_spec,\n",
    "                                 attack_types=attack_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(attacks_result.summary(by_slices=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_privacy.privacy.membership_inference_attack.plotting as plotting\n",
    "plotting.plot_roc_curve(attacks_result.get_result_with_max_auc().roc_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"attack_dmae.txt\", \"w\") as text_file:\n",
    "#    text_file.write(str(attacks_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attacks_result.calculate_pd_dataframe().to_csv('attack_notrain5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow-2.15.0]",
   "language": "python",
   "name": "conda-env-tensorflow-2.15.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
